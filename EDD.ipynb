{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465d49b2-58b5-4796-b1ec-c9105396b647",
   "metadata": {},
   "source": [
    "# ## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e49499-5736-478a-abd6-383455b46b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225a0bfb-4b6b-4116-81b1-a0715bb0a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a8f599-b95a-4ac5-ba9e-a0ea968044ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set up plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec365ca-dc2a-4bdd-86b4-fdcd75b2a44a",
   "metadata": {},
   "source": [
    "# ## 2. Database Connection and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9981ae54-59c2-47cd-aa1d-5ba2fb8f8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection parameters\n",
    "DB_HOST = \"localhost\"  \n",
    "DB_PORT = \"5432\"       \n",
    "DB_NAME = \"ward_data\"  \n",
    "DB_USER = \"kshitizbhatnagar\" \n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create a connection to the PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        connection_string = f\"postgresql://{DB_USER}:@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        print(\"Successfully connected to PostgreSQL database!\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_postgres():\n",
    "    \"\"\"Load the hospital data from PostgreSQL database\"\"\"\n",
    "    engine = get_db_connection()\n",
    "    if engine is None:\n",
    "        raise Exception(\"Failed to connect to database\")\n",
    "    \n",
    "    # Query to load data from hospital_data table\n",
    "    query = \"SELECT * FROM hospital_data\"\n",
    "    \n",
    "    # Load data into pandas DataFrame\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} records from PostgreSQL database\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e0dbaa-00e3-4bae-9319-17c75c5932b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to PostgreSQL database!\n",
      "Loaded 14001 records from PostgreSQL database\n",
      "Dataset shape: (14001, 22)\n",
      "\n",
      "Dataset columns: ['mrn', 'age', 'gender', 'visit_id', 'comorbidities', 'num_comorbidities', 'visit_date', 'expected_discharge_date', 'actual_discharge_date', 'actual_los', 'primary_diagnosis', 'severity_score', 'heart_rate', 'systolic_bp', 'temperature', 'oxygen_saturation', 'is_surgical', 'ward_occupancy_pct', 'laboratory_report_time', 'pharmacy_billing_time', 'insurance_claim_settlement_time', 'discharge_summary_time']\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14001 entries, 0 to 14000\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype         \n",
      "---  ------                           --------------  -----         \n",
      " 0   mrn                              14001 non-null  object        \n",
      " 1   age                              14001 non-null  int64         \n",
      " 2   gender                           14001 non-null  object        \n",
      " 3   visit_id                         14001 non-null  object        \n",
      " 4   comorbidities                    14001 non-null  object        \n",
      " 5   num_comorbidities                14001 non-null  int64         \n",
      " 6   visit_date                       14001 non-null  datetime64[ns]\n",
      " 7   expected_discharge_date          14001 non-null  object        \n",
      " 8   actual_discharge_date            14001 non-null  datetime64[ns]\n",
      " 9   actual_los                       14001 non-null  int64         \n",
      " 10  primary_diagnosis                14001 non-null  object        \n",
      " 11  severity_score                   14001 non-null  int64         \n",
      " 12  heart_rate                       14001 non-null  float64       \n",
      " 13  systolic_bp                      14001 non-null  float64       \n",
      " 14  temperature                      14001 non-null  float64       \n",
      " 15  oxygen_saturation                14001 non-null  float64       \n",
      " 16  is_surgical                      14001 non-null  object        \n",
      " 17  ward_occupancy_pct               14001 non-null  float64       \n",
      " 18  laboratory_report_time           14001 non-null  object        \n",
      " 19  pharmacy_billing_time            14001 non-null  object        \n",
      " 20  insurance_claim_settlement_time  14001 non-null  object        \n",
      " 21  discharge_summary_time           14001 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(5), int64(4), object(11)\n",
      "memory usage: 2.4+ MB\n",
      "\n",
      "Missing values:\n",
      "mrn                                0\n",
      "age                                0\n",
      "gender                             0\n",
      "visit_id                           0\n",
      "comorbidities                      0\n",
      "num_comorbidities                  0\n",
      "visit_date                         0\n",
      "expected_discharge_date            0\n",
      "actual_discharge_date              0\n",
      "actual_los                         0\n",
      "primary_diagnosis                  0\n",
      "severity_score                     0\n",
      "heart_rate                         0\n",
      "systolic_bp                        0\n",
      "temperature                        0\n",
      "oxygen_saturation                  0\n",
      "is_surgical                        0\n",
      "ward_occupancy_pct                 0\n",
      "laboratory_report_time             0\n",
      "pharmacy_billing_time              0\n",
      "insurance_claim_settlement_time    0\n",
      "discharge_summary_time             0\n",
      "dtype: int64\n",
      "\n",
      "Summary statistics:\n",
      "            age  num_comorbidities                     visit_date  \\\n",
      "count 14001.000          14001.000                          14001   \n",
      "mean     54.471              1.546  2024-10-31 02:11:06.316691712   \n",
      "min      18.000              0.000            2024-05-08 19:13:00   \n",
      "25%      42.000              1.000            2024-08-04 01:10:00   \n",
      "50%      54.000              1.000            2024-10-31 04:57:00   \n",
      "75%      67.000              2.000            2025-01-26 22:13:00   \n",
      "max      99.000              4.000            2025-05-03 16:46:00   \n",
      "std      17.535              1.117                            NaN   \n",
      "\n",
      "               actual_discharge_date  actual_los  severity_score  heart_rate  \\\n",
      "count                          14001   14001.000       14001.000   14001.000   \n",
      "mean   2024-11-09 15:51:44.628240896       9.570           2.993      86.923   \n",
      "min              2024-05-10 08:06:00       1.000           1.000      43.910   \n",
      "25%              2024-08-13 20:04:00       5.000           2.000      78.535   \n",
      "50%              2024-11-09 18:53:00       8.000           3.000      86.816   \n",
      "75%              2025-02-04 22:09:00      12.000           4.000      95.311   \n",
      "max              2025-05-08 13:26:00      58.000           5.000     136.423   \n",
      "std                              NaN       7.353           1.423      12.294   \n",
      "\n",
      "       systolic_bp  temperature  oxygen_saturation  ward_occupancy_pct  \n",
      "count    14001.000    14001.000          14001.000           14001.000  \n",
      "mean       112.937       37.600             94.992              86.581  \n",
      "min         70.000       35.400             86.111              75.000  \n",
      "25%        102.466       37.207             93.320              80.859  \n",
      "50%        112.915       37.602             95.010              86.587  \n",
      "75%        123.311       37.987             96.693              92.278  \n",
      "max        168.270       40.000            100.000              97.997  \n",
      "std         15.515        0.577              2.414               6.630  \n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = load_data_from_postgres()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nDataset columns:\", df.columns.tolist())\n",
    "print(\"\\nDataset info:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d9cac2-fb5a-44fd-a0ce-ed1aefa37a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mrn</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>visit_id</th>\n",
       "      <th>comorbidities</th>\n",
       "      <th>num_comorbidities</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>expected_discharge_date</th>\n",
       "      <th>actual_discharge_date</th>\n",
       "      <th>actual_los</th>\n",
       "      <th>primary_diagnosis</th>\n",
       "      <th>severity_score</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>systolic_bp</th>\n",
       "      <th>temperature</th>\n",
       "      <th>oxygen_saturation</th>\n",
       "      <th>is_surgical</th>\n",
       "      <th>ward_occupancy_pct</th>\n",
       "      <th>laboratory_report_time</th>\n",
       "      <th>pharmacy_billing_time</th>\n",
       "      <th>insurance_claim_settlement_time</th>\n",
       "      <th>discharge_summary_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>63</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuberculosis,Hypertension</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-31 21:25:00</td>\n",
       "      <td>2025-02-08</td>\n",
       "      <td>2025-02-06 21:25:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Malaria</td>\n",
       "      <td>2</td>\n",
       "      <td>84.331</td>\n",
       "      <td>117.770</td>\n",
       "      <td>38.131</td>\n",
       "      <td>99.077</td>\n",
       "      <td>Yes</td>\n",
       "      <td>75.530</td>\n",
       "      <td>27:58:41</td>\n",
       "      <td>16:14:46</td>\n",
       "      <td>85:50:43</td>\n",
       "      <td>08:51:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-01 21:59:00</td>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>2024-08-07 21:59:00</td>\n",
       "      <td>6</td>\n",
       "      <td>Heart Failure</td>\n",
       "      <td>4</td>\n",
       "      <td>91.865</td>\n",
       "      <td>94.134</td>\n",
       "      <td>38.211</td>\n",
       "      <td>91.558</td>\n",
       "      <td>No</td>\n",
       "      <td>86.962</td>\n",
       "      <td>42:03:53</td>\n",
       "      <td>18:25:43</td>\n",
       "      <td>57:09:35</td>\n",
       "      <td>09:13:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002</td>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-07-06 21:45:00</td>\n",
       "      <td>2024-07-14</td>\n",
       "      <td>2024-07-15 21:45:00</td>\n",
       "      <td>9</td>\n",
       "      <td>COPD Exacerbation</td>\n",
       "      <td>4</td>\n",
       "      <td>86.619</td>\n",
       "      <td>87.351</td>\n",
       "      <td>38.161</td>\n",
       "      <td>92.313</td>\n",
       "      <td>Yes</td>\n",
       "      <td>84.738</td>\n",
       "      <td>01:43:07</td>\n",
       "      <td>11:50:16</td>\n",
       "      <td>31:34:34</td>\n",
       "      <td>02:58:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002</td>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-20 11:52:00</td>\n",
       "      <td>2025-03-26</td>\n",
       "      <td>2025-03-25 11:52:00</td>\n",
       "      <td>5</td>\n",
       "      <td>GI Bleed</td>\n",
       "      <td>5</td>\n",
       "      <td>105.125</td>\n",
       "      <td>127.344</td>\n",
       "      <td>37.964</td>\n",
       "      <td>95.007</td>\n",
       "      <td>Yes</td>\n",
       "      <td>91.405</td>\n",
       "      <td>01:04:25</td>\n",
       "      <td>23:13:38</td>\n",
       "      <td>50:08:14</td>\n",
       "      <td>11:41:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000003</td>\n",
       "      <td>29</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>Diabetes,Chronic Kidney Disease,Coronary Arter...</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-03 14:48:00</td>\n",
       "      <td>2025-04-08</td>\n",
       "      <td>2025-04-09 14:48:00</td>\n",
       "      <td>6</td>\n",
       "      <td>UTI</td>\n",
       "      <td>4</td>\n",
       "      <td>101.154</td>\n",
       "      <td>114.931</td>\n",
       "      <td>37.535</td>\n",
       "      <td>95.027</td>\n",
       "      <td>No</td>\n",
       "      <td>87.405</td>\n",
       "      <td>28:24:38</td>\n",
       "      <td>01:00:56</td>\n",
       "      <td>50:56:51</td>\n",
       "      <td>10:02:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mrn  age gender visit_id  \\\n",
       "0  000001   63      F        1   \n",
       "1  000002   57      M        2   \n",
       "2  000002   57      M        3   \n",
       "3  000002   57      M        4   \n",
       "4  000003   29      M        5   \n",
       "\n",
       "                                       comorbidities  num_comorbidities  \\\n",
       "0                          Tuberculosis,Hypertension                  2   \n",
       "1                                               None                  0   \n",
       "2                                               None                  0   \n",
       "3                                               None                  0   \n",
       "4  Diabetes,Chronic Kidney Disease,Coronary Arter...                  3   \n",
       "\n",
       "           visit_date expected_discharge_date actual_discharge_date  \\\n",
       "0 2025-01-31 21:25:00              2025-02-08   2025-02-06 21:25:00   \n",
       "1 2024-08-01 21:59:00              2024-08-08   2024-08-07 21:59:00   \n",
       "2 2024-07-06 21:45:00              2024-07-14   2024-07-15 21:45:00   \n",
       "3 2025-03-20 11:52:00              2025-03-26   2025-03-25 11:52:00   \n",
       "4 2025-04-03 14:48:00              2025-04-08   2025-04-09 14:48:00   \n",
       "\n",
       "   actual_los  primary_diagnosis  severity_score  heart_rate  systolic_bp  \\\n",
       "0           6            Malaria               2      84.331      117.770   \n",
       "1           6      Heart Failure               4      91.865       94.134   \n",
       "2           9  COPD Exacerbation               4      86.619       87.351   \n",
       "3           5           GI Bleed               5     105.125      127.344   \n",
       "4           6                UTI               4     101.154      114.931   \n",
       "\n",
       "   temperature  oxygen_saturation is_surgical  ward_occupancy_pct  \\\n",
       "0       38.131             99.077         Yes              75.530   \n",
       "1       38.211             91.558          No              86.962   \n",
       "2       38.161             92.313         Yes              84.738   \n",
       "3       37.964             95.007         Yes              91.405   \n",
       "4       37.535             95.027          No              87.405   \n",
       "\n",
       "  laboratory_report_time pharmacy_billing_time  \\\n",
       "0               27:58:41              16:14:46   \n",
       "1               42:03:53              18:25:43   \n",
       "2               01:43:07              11:50:16   \n",
       "3               01:04:25              23:13:38   \n",
       "4               28:24:38              01:00:56   \n",
       "\n",
       "  insurance_claim_settlement_time discharge_summary_time  \n",
       "0                        85:50:43               08:51:56  \n",
       "1                        57:09:35               09:13:22  \n",
       "2                        31:34:34               02:58:04  \n",
       "3                        50:08:14               11:41:59  \n",
       "4                        50:56:51               10:02:38  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3051a8c-ec37-46b1-b765-7669374e9cba",
   "metadata": {},
   "source": [
    "# ## 3. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23ef660-64bb-41d1-98c6-b73c23262cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for modeling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Raw data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_processed : pandas DataFrame\n",
    "        Processed data ready for modeling\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df_processed['visit_date'] = pd.to_datetime(df_processed['visit_date'])\n",
    "    df_processed['actual_discharge_date'] = pd.to_datetime(df_processed['actual_discharge_date'])\n",
    "    \n",
    "    # Extract useful datetime features\n",
    "    df_processed['visit_year'] = df_processed['visit_date'].dt.year\n",
    "    df_processed['visit_month'] = df_processed['visit_date'].dt.month\n",
    "    df_processed['visit_day'] = df_processed['visit_date'].dt.day\n",
    "    df_processed['visit_hour'] = df_processed['visit_date'].dt.hour\n",
    "    df_processed['visit_weekday'] = df_processed['visit_date'].dt.weekday\n",
    "    df_processed['visit_is_weekend'] = df_processed['visit_weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # Convert time fields to hours\n",
    "    time_cols = ['laboratory_report_time', 'pharmacy_billing_time', \n",
    "                'insurance_claim_settlement_time', 'discharge_summary_time']\n",
    "    \n",
    "    for col in time_cols:\n",
    "        df_processed[f'{col}_hours'] = df_processed[col].apply(time_to_hours)\n",
    "        df_processed.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Process comorbidities\n",
    "    # Extract the top comorbidities\n",
    "    top_comorbidities = extract_top_comorbidities(df_processed, n=10)\n",
    "    \n",
    "    # Create binary flags for each top comorbidity\n",
    "    for comorbidity in top_comorbidities:\n",
    "        df_processed[f'has_{comorbidity.lower().replace(\" \", \"_\")}'] = df_processed['comorbidities'].apply(\n",
    "            lambda x: 1 if comorbidity in str(x) else 0\n",
    "        )\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    df_processed['gender_encoded'] = df_processed['gender'].map({'M': 1, 'F': 0})\n",
    "    df_processed['is_surgical_encoded'] = df_processed['is_surgical'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Encode primary diagnosis - use a diagnosis-specific approach\n",
    "    diagnosis_stats = create_diagnosis_mapping(df_processed)\n",
    "    df_processed['diagnosis_avg_los'] = df_processed['primary_diagnosis'].map(diagnosis_stats['avg_los'])\n",
    "    df_processed['diagnosis_std_los'] = df_processed['primary_diagnosis'].map(diagnosis_stats['std_los'])\n",
    "    df_processed['diagnosis_frequency'] = df_processed['primary_diagnosis'].map(diagnosis_stats['frequency'])\n",
    "    \n",
    "    # Create interaction features\n",
    "    df_processed['age_severity'] = df_processed['age'] * df_processed['severity_score']\n",
    "    df_processed['age_comorbidities'] = df_processed['age'] * df_processed['num_comorbidities']\n",
    "    \n",
    "    # Drop columns not needed for modeling\n",
    "    columns_to_drop = ['comorbidities', 'gender', 'is_surgical', 'expected_discharge_date', \n",
    "                      'actual_discharge_date', 'primary_diagnosis', 'visit_id']\n",
    "    \n",
    "    df_processed.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    return df_processed, top_comorbidities, diagnosis_stats\n",
    "\n",
    "def time_to_hours(time_str):\n",
    "    \"\"\"Convert time string in format HH:MM:SS to decimal hours\"\"\"\n",
    "    try:\n",
    "        parts = time_str.split(':')\n",
    "        if len(parts) != 3:\n",
    "            return 0\n",
    "        hours = int(parts[0])\n",
    "        minutes = int(parts[1])\n",
    "        seconds = int(parts[2])\n",
    "        return hours + minutes/60 + seconds/3600\n",
    "    except Exception:\n",
    "        return 0  # Default to 0 if there's an error\n",
    "\n",
    "def extract_top_comorbidities(df, n=10):\n",
    "    \"\"\"\n",
    "    Extract the top n most common comorbidities from the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Dataset containing a 'comorbidities' column\n",
    "    n : int\n",
    "        Number of top comorbidities to extract\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    top_comorbidities : list\n",
    "        List of the top n comorbidities\n",
    "    \"\"\"\n",
    "    # Initialize counter for comorbidities\n",
    "    comorbidity_counts = {}\n",
    "    \n",
    "    # Count each comorbidity\n",
    "    for comorbidities in df['comorbidities']:\n",
    "        if comorbidities == 'None':\n",
    "            continue\n",
    "            \n",
    "        for comorbidity in comorbidities.split(','):\n",
    "            comorbidity = comorbidity.strip()\n",
    "            if comorbidity not in comorbidity_counts:\n",
    "                comorbidity_counts[comorbidity] = 0\n",
    "            comorbidity_counts[comorbidity] += 1\n",
    "    \n",
    "    # Sort comorbidities by frequency\n",
    "    sorted_comorbidities = sorted(comorbidity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top n comorbidities\n",
    "    return [item[0] for item in sorted_comorbidities[:n]]\n",
    "\n",
    "def create_diagnosis_mapping(df):\n",
    "    \"\"\"\n",
    "    Create detailed statistics for each diagnosis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Dataset containing 'primary_diagnosis' and 'actual_los' columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    diagnosis_stats : dict\n",
    "        Dictionary with statistics for each diagnosis\n",
    "    \"\"\"\n",
    "    # Group by diagnosis and calculate statistics\n",
    "    diagnosis_groups = df.groupby('primary_diagnosis')['actual_los']\n",
    "    \n",
    "    # Calculate various statistics\n",
    "    avg_los = diagnosis_groups.mean()\n",
    "    median_los = diagnosis_groups.median()\n",
    "    std_los = diagnosis_groups.std()\n",
    "    min_los = diagnosis_groups.min()\n",
    "    max_los = diagnosis_groups.max()\n",
    "    frequency = diagnosis_groups.count()\n",
    "    \n",
    "    # Create a comprehensive mapping\n",
    "    diagnosis_stats = {\n",
    "        'avg_los': avg_los.to_dict(),\n",
    "        'median_los': median_los.to_dict(),\n",
    "        'std_los': std_los.to_dict(),\n",
    "        'min_los': min_los.to_dict(),\n",
    "        'max_los': max_los.to_dict(),\n",
    "        'frequency': frequency.to_dict()\n",
    "    }\n",
    "    \n",
    "    return diagnosis_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cafd11-3483-463a-bab7-f4d0a5cea2e8",
   "metadata": {},
   "source": [
    "# ## 4. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db08249-1e53-4718-a68c-5098b566a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(X_train, y_train, X_val, feature_selection_method='mutual_info', k=15):\n",
    "    \"\"\"\n",
    "    Perform automated feature engineering and selection\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Training features\n",
    "    y_train : pandas Series\n",
    "        Training target\n",
    "    X_val : pandas DataFrame\n",
    "        Validation features\n",
    "    feature_selection_method : str\n",
    "        Method to use for feature selection ('mutual_info' or 'f_regression')\n",
    "    k : int\n",
    "        Number of features to select\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_selected : pandas DataFrame\n",
    "        Training features after selection\n",
    "    X_val_selected : pandas DataFrame\n",
    "        Validation features after selection\n",
    "    selected_features : list\n",
    "        Names of selected features\n",
    "    selector : object\n",
    "        Trained feature selector\n",
    "    preprocessor : object\n",
    "        Trained preprocessor\n",
    "    \"\"\"\n",
    "    # Create numerical and categorical feature lists\n",
    "    categorical_features = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "    numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_val_preprocessed = preprocessor.transform(X_val)\n",
    "    \n",
    "    # Feature selection\n",
    "    if feature_selection_method == 'mutual_info':\n",
    "        selector = SelectKBest(mutual_info_regression, k=min(k, X_train_preprocessed.shape[1]))\n",
    "    else:\n",
    "        selector = SelectKBest(f_regression, k=min(k, X_train_preprocessed.shape[1]))\n",
    "    \n",
    "    X_train_selected = selector.fit_transform(X_train_preprocessed, y_train)\n",
    "    X_val_selected = selector.transform(X_val_preprocessed)\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    selected_indices = selector.get_support(indices=True)\n",
    "    \n",
    "    # Handle numerical features\n",
    "    numeric_feature_indices = []\n",
    "    for i, feature in enumerate(numerical_features):\n",
    "        numeric_feature_indices.append((i, feature))\n",
    "        \n",
    "    # Handle categorical features with one-hot encoding\n",
    "    cat_feature_indices = []\n",
    "    if categorical_features:\n",
    "        encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_feature_names = []\n",
    "        \n",
    "        for i, feature in enumerate(categorical_features):\n",
    "            categories = encoder.categories_[i]\n",
    "            for cat in categories:\n",
    "                cat_feature_names.append(f\"{feature}_{cat}\")\n",
    "                \n",
    "        start_idx = len(numerical_features)\n",
    "        for i, feature_name in enumerate(cat_feature_names):\n",
    "            cat_feature_indices.append((start_idx + i, feature_name))\n",
    "    \n",
    "    # Combine all feature indices\n",
    "    all_feature_indices = numeric_feature_indices + cat_feature_indices\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = [feature_name for idx, feature_name in all_feature_indices if idx in selected_indices]\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features: {selected_features}\")\n",
    "    \n",
    "    return X_train_selected, X_val_selected, selected_features, selector, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026cea7-7296-4671-aeba-f62a0fcd509a",
   "metadata": {},
   "source": [
    "# ## 5. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716541d3-b89a-4d39-a031-cc618264e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple regression models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        Training features\n",
    "    y_train : numpy array or pandas Series\n",
    "        Training target\n",
    "    X_val : numpy array or pandas DataFrame\n",
    "        Validation features\n",
    "    y_val : numpy array or pandas Series\n",
    "        Validation target\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary with model names as keys and performance metrics as values\n",
    "    models : dict\n",
    "        Dictionary with model names as keys and fitted model objects as values\n",
    "    \"\"\"\n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'Lasso': Lasso(),\n",
    "        'ElasticNet': ElasticNet(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "        'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf'),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    # Results dictionary to store performance metrics\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Make predictions on validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    # Sort models by MAE\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['MAE'])\n",
    "    \n",
    "    print(\"\\nModels ranked by MAE:\")\n",
    "    for i, (name, metrics) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. {name} - MAE: {metrics['MAE']:.2f}, RMSE: {metrics['RMSE']:.2f}, R2: {metrics['R2']:.4f}\")\n",
    "    \n",
    "    return results, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcbd9a-a712-45d3-832d-59daf105a4c1",
   "metadata": {},
   "source": [
    "# ## 6. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "028a85d2-959d-4d94-bd2c-7d7515d47f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(best_model_name, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters for the best model using Optuna\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    best_model_name : str\n",
    "        Name of the best model to tune\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        Training features\n",
    "    y_train : numpy array or pandas Series\n",
    "        Training target\n",
    "    X_val : numpy array or pandas DataFrame\n",
    "        Validation features\n",
    "    y_val : numpy array or pandas Series\n",
    "        Validation target\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : object\n",
    "        Trained model with optimized hyperparameters\n",
    "    best_params : dict\n",
    "        Best hyperparameters\n",
    "    \"\"\"\n",
    "    # Define objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Model-specific hyperparameter search spaces\n",
    "        if best_model_name == 'Linear Regression':\n",
    "            # Linear Regression doesn't have hyperparameters to tune\n",
    "            model = LinearRegression()\n",
    "        \n",
    "        elif best_model_name == 'Ridge':\n",
    "            alpha = trial.suggest_float('alpha', 0.01, 10.0, log=True)\n",
    "            model = Ridge(alpha=alpha)\n",
    "        \n",
    "        elif best_model_name == 'Lasso':\n",
    "            alpha = trial.suggest_float('alpha', 0.001, 1.0, log=True)\n",
    "            model = Lasso(alpha=alpha)\n",
    "        \n",
    "        elif best_model_name == 'ElasticNet':\n",
    "            alpha = trial.suggest_float('alpha', 0.001, 1.0, log=True)\n",
    "            l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n",
    "            model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        \n",
    "        elif best_model_name == 'Random Forest':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            \n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        elif best_model_name == 'Gradient Boosting':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "            \n",
    "            model = GradientBoostingRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                subsample=subsample,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        elif best_model_name == 'XGBoost':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "            colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "            \n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        elif best_model_name == 'LightGBM':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "            colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "            \n",
    "            model = lgb.LGBMRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        elif best_model_name == 'SVR':\n",
    "            C = trial.suggest_float('C', 0.1, 100.0, log=True)\n",
    "            epsilon = trial.suggest_float('epsilon', 0.01, 1.0, log=True)\n",
    "            gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "            \n",
    "            model = SVR(C=C, epsilon=epsilon, gamma=gamma)\n",
    "        \n",
    "        elif best_model_name == 'KNN':\n",
    "            n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n",
    "            weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            p = trial.suggest_int('p', 1, 2)  # Manhattan or Euclidean distance\n",
    "            \n",
    "            model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, p=p)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {best_model_name}\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        \n",
    "        return mae\n",
    "    \n",
    "    # Create and run study\n",
    "    print(f\"Tuning hyperparameters for {best_model_name}...\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    # Recreate and train the model with the best parameters\n",
    "    if best_model_name == 'Linear Regression':\n",
    "        best_model = LinearRegression()\n",
    "    \n",
    "    elif best_model_name == 'Ridge':\n",
    "        best_model = Ridge(alpha=best_params['alpha'])\n",
    "    \n",
    "    elif best_model_name == 'Lasso':\n",
    "        best_model = Lasso(alpha=best_params['alpha'])\n",
    "    \n",
    "    elif best_model_name == 'ElasticNet':\n",
    "        best_model = ElasticNet(alpha=best_params['alpha'], l1_ratio=best_params['l1_ratio'])\n",
    "    \n",
    "    elif best_model_name == 'Random Forest':\n",
    "        best_model = RandomForestRegressor(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            min_samples_split=best_params['min_samples_split'],\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        best_model = GradientBoostingRegressor(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            subsample=best_params['subsample'],\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb.XGBRegressor(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            subsample=best_params['subsample'],\n",
    "            colsample_bytree=best_params['colsample_bytree'],\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif best_model_name == 'LightGBM':\n",
    "        best_model = lgb.LGBMRegressor(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            subsample=best_params['subsample'],\n",
    "            colsample_bytree=best_params['colsample_bytree'],\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif best_model_name == 'SVR':\n",
    "        best_model = SVR(\n",
    "            C=best_params['C'],\n",
    "            epsilon=best_params['epsilon'],\n",
    "            gamma=best_params['gamma']\n",
    "        )\n",
    "    \n",
    "    elif best_model_name == 'KNN':\n",
    "        best_model = KNeighborsRegressor(\n",
    "            n_neighbors=best_params['n_neighbors'],\n",
    "            weights=best_params['weights'],\n",
    "            p=best_params['p']\n",
    "        )\n",
    "    \n",
    "    # Train the best model\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the best model\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"Tuned {best_model_name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3d597-9335-4073-ae0b-ca11078e81b5",
   "metadata": {},
   "source": [
    "# ## 7. ML Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf453bc-fb12-4bc3-8ea9-9df6758d61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HospitalLOSPredictor:\n",
    "    \"\"\"\n",
    "    End-to-end ML pipeline for predicting hospital length of stay\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.preprocessor = None\n",
    "        self.feature_selector = None\n",
    "        self.model = None\n",
    "        self.selected_features = None\n",
    "        self.top_comorbidities = None\n",
    "        self.diagnosis_stats = None\n",
    "        self.model_type = None\n",
    "        \n",
    "    def fit(self, db_connection_params=None, data=None):\n",
    "        \"\"\"\n",
    "        Train the entire pipeline\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        db_connection_params : dict\n",
    "            Parameters for connecting to the database\n",
    "        data : pandas DataFrame\n",
    "            Data for training the model (if not loading from database)\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        print(\"Loading data...\")\n",
    "        if data is not None:\n",
    "            df = data\n",
    "        elif db_connection_params is not None:\n",
    "            # Connect to database\n",
    "            connection_string = f\"postgresql://{db_connection_params['user']}:@{db_connection_params['host']}:{db_connection_params['port']}/{db_connection_params['database']}\"\n",
    "            engine = create_engine(connection_string)\n",
    "            \n",
    "            # Load data\n",
    "            query = \"SELECT * FROM hospital_data\"\n",
    "            df = pd.read_sql_query(query, engine)\n",
    "            \n",
    "            # Convert date columns to datetime\n",
    "            if 'visit_date' in df.columns:\n",
    "                df['visit_date'] = pd.to_datetime(df['visit_date'])\n",
    "            if 'actual_discharge_date' in df.columns:\n",
    "                df['actual_discharge_date'] = pd.to_datetime(df['actual_discharge_date'])\n",
    "            \n",
    "            # Create date features and drop datetime columns to avoid type issues\n",
    "            if 'visit_date' in df.columns:\n",
    "                df['visit_year'] = df['visit_date'].dt.year\n",
    "                df['visit_month'] = df['visit_date'].dt.month\n",
    "                df['visit_day'] = df['visit_date'].dt.day\n",
    "                df['visit_hour'] = df['visit_date'].dt.hour\n",
    "                df['visit_weekday'] = df['visit_date'].dt.weekday\n",
    "                df['visit_is_weekend'] = df['visit_weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "                df = df.drop(columns=['visit_date'])\n",
    "            \n",
    "            # Handle other datetime columns\n",
    "            if 'actual_discharge_date' in df.columns:\n",
    "                df = df.drop(columns=['actual_discharge_date'])\n",
    "            if 'expected_discharge_date' in df.columns:\n",
    "                df = df.drop(columns=['expected_discharge_date'])\n",
    "        else:\n",
    "            raise ValueError(\"Either db_connection_params or data must be provided\")\n",
    "        \n",
    "        print(f\"Loaded {len(df)} records\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        print(\"Preprocessing data...\")\n",
    "        # Extract comorbidities\n",
    "        self.top_comorbidities = self._extract_top_comorbidities(df, n=10)\n",
    "        \n",
    "        # Extract diagnosis stats\n",
    "        self.diagnosis_stats = self._create_diagnosis_stats(df)\n",
    "        \n",
    "        # Process time fields if they exist\n",
    "        time_cols = ['laboratory_report_time', 'pharmacy_billing_time', \n",
    "                    'insurance_claim_settlement_time', 'discharge_summary_time']\n",
    "        \n",
    "        for col in time_cols:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_hours'] = df[col].apply(self._time_to_hours)\n",
    "                df = df.drop(columns=[col])\n",
    "        \n",
    "        # Create binary flags for comorbidities\n",
    "        if 'comorbidities' in df.columns:\n",
    "            for comorbidity in self.top_comorbidities:\n",
    "                df[f'has_{comorbidity.lower().replace(\" \", \"_\")}'] = df['comorbidities'].apply(\n",
    "                    lambda x: 1 if comorbidity in str(x) else 0\n",
    "                )\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        if 'gender' in df.columns:\n",
    "            df['gender_encoded'] = df['gender'].map({'M': 1, 'F': 0})\n",
    "            df = df.drop(columns=['gender'])\n",
    "        \n",
    "        if 'is_surgical' in df.columns:\n",
    "            df['is_surgical_encoded'] = df['is_surgical'].map({'Yes': 1, 'No': 0})\n",
    "            df = df.drop(columns=['is_surgical'])\n",
    "        \n",
    "        # Add diagnosis features\n",
    "        if 'primary_diagnosis' in df.columns:\n",
    "            df['diagnosis_avg_los'] = df['primary_diagnosis'].map(self.diagnosis_stats['avg_los'])\n",
    "            df['diagnosis_std_los'] = df['primary_diagnosis'].map(self.diagnosis_stats['std_los'])\n",
    "            df['diagnosis_frequency'] = df['primary_diagnosis'].map(self.diagnosis_stats['frequency'])\n",
    "        \n",
    "        # Create interaction features\n",
    "        if 'age' in df.columns and 'severity_score' in df.columns:\n",
    "            df['age_severity'] = df['age'] * df['severity_score']\n",
    "        \n",
    "        if 'age' in df.columns and 'num_comorbidities' in df.columns:\n",
    "            df['age_comorbidities'] = df['age'] * df['num_comorbidities']\n",
    "        \n",
    "        # Define features and target\n",
    "        print(\"Defining features and target...\")\n",
    "        # Make sure actual_los and mrn exist in the dataframe\n",
    "        if 'actual_los' not in df.columns or 'mrn' not in df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'actual_los' and 'mrn' columns\")\n",
    "        \n",
    "        X = df.drop(columns=['actual_los', 'mrn'])\n",
    "        y = df['actual_los']\n",
    "        \n",
    "        # Split data\n",
    "        print(\"Splitting data...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "        \n",
    "        # Separate numerical and categorical features\n",
    "        categorical_features = [col for col in X.columns if X[col].dtype == 'object']\n",
    "        numerical_features = [col for col in X.columns if X[col].dtype != 'object' and col != 'gender_encoded']\n",
    "        binary_features = ['gender_encoded'] if 'gender_encoded' in X.columns else []\n",
    "        binary_features += [col for col in X.columns if col.startswith('has_') or col == 'is_surgical_encoded']\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        print(\"Creating preprocessing pipeline...\")\n",
    "        transformers = []\n",
    "        \n",
    "        if numerical_features:\n",
    "            transformers.append(('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_features))\n",
    "        \n",
    "        if categorical_features:\n",
    "            transformers.append(('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_features))\n",
    "        \n",
    "        if binary_features:\n",
    "            transformers.append(('bin', 'passthrough', binary_features))\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        \n",
    "        # Fit and transform the data\n",
    "        print(\"Fitting preprocessor...\")\n",
    "        X_train_preprocessed = self.preprocessor.fit_transform(X_train)\n",
    "        X_val_preprocessed = self.preprocessor.transform(X_val)\n",
    "        \n",
    "        # Feature selection\n",
    "        print(\"Performing feature selection...\")\n",
    "        self.feature_selector = SelectKBest(mutual_info_regression, k=min(15, X_train_preprocessed.shape[1]))\n",
    "        X_train_selected = self.feature_selector.fit_transform(X_train_preprocessed, y_train)\n",
    "        X_val_selected = self.feature_selector.transform(X_val_preprocessed)\n",
    "        \n",
    "        # Get approximate feature names (this is simplified)\n",
    "        self.selected_features = [f\"feature_{i}\" for i in range(X_train_selected.shape[1])]\n",
    "        \n",
    "        # Transform test set\n",
    "        X_test_preprocessed = self.preprocessor.transform(X_test)\n",
    "        X_test_selected = self.feature_selector.transform(X_test_preprocessed)\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        print(\"Training and evaluating models...\")\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge': Ridge(),\n",
    "            'Lasso': Lasso(),\n",
    "            'ElasticNet': ElasticNet(),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "            'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "            'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42),\n",
    "            'SVR': SVR(kernel='rbf'),\n",
    "            'KNN': KNeighborsRegressor(n_neighbors=5)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        trained_models = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X_train_selected, y_train)\n",
    "            trained_models[name] = model\n",
    "            \n",
    "            # Make predictions on validation set\n",
    "            y_pred = model.predict(X_val_selected)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'MAE': mae,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'R2': r2\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}\")\n",
    "        \n",
    "        # Sort models by MAE (lower is better)\n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1]['MAE'])\n",
    "        \n",
    "        print(\"\\nModels ranked by MAE:\")\n",
    "        for i, (name, metrics) in enumerate(sorted_results, 1):\n",
    "            print(f\"{i}. {name} - MAE: {metrics['MAE']:.2f}, RMSE: {metrics['RMSE']:.2f}, R2: {metrics['R2']:.4f}\")\n",
    "        \n",
    "        # Select the best model\n",
    "        best_model_name, best_metrics = sorted_results[0]\n",
    "        self.model_type = best_model_name\n",
    "        self.model = trained_models[best_model_name]\n",
    "        \n",
    "        print(f\"\\n=== SELECTED MODEL: {best_model_name} ===\")\n",
    "        print(f\"Best model metrics - MAE: {best_metrics['MAE']:.2f}, RMSE: {best_metrics['RMSE']:.2f}, R2: {best_metrics['R2']:.4f}\")\n",
    "\n",
    "        # Add this cell to visualize model performances\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        # Extract model performances\n",
    "        model_names = [name for name, _ in sorted_results]\n",
    "        mae_values = [metrics['MAE'] for _, metrics in sorted_results]\n",
    "        rmse_values = [metrics['RMSE'] for _, metrics in sorted_results]\n",
    "        r2_values = [metrics['R2'] for _, metrics in sorted_results]\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "        \n",
    "        # Plot MAE\n",
    "        axes[0].bar(model_names, mae_values, color='skyblue')\n",
    "        axes[0].set_title('Mean Absolute Error (MAE)')\n",
    "        axes[0].set_xlabel('Model')\n",
    "        axes[0].set_ylabel('MAE (days)')\n",
    "        axes[0].set_ylim(0, max(mae_values) * 1.1)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        for i, v in enumerate(mae_values):\n",
    "            axes[0].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "        \n",
    "        # Plot RMSE\n",
    "        axes[1].bar(model_names, rmse_values, color='lightgreen')\n",
    "        axes[1].set_title('Root Mean Squared Error (RMSE)')\n",
    "        axes[1].set_xlabel('Model')\n",
    "        axes[1].set_ylabel('RMSE (days)')\n",
    "        axes[1].set_ylim(0, max(rmse_values) * 1.1)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        for i, v in enumerate(rmse_values):\n",
    "            axes[1].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "        \n",
    "        # Plot R2\n",
    "        axes[2].bar(model_names, r2_values, color='salmon')\n",
    "        axes[2].set_title('R Score')\n",
    "        axes[2].set_xlabel('Model')\n",
    "        axes[2].set_ylabel('R')\n",
    "        axes[2].set_ylim(0, max(r2_values) * 1.1)\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "        for i, v in enumerate(r2_values):\n",
    "            axes[2].text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a detailed performance table\n",
    "        performance_df = pd.DataFrame({\n",
    "            'Model': model_names,\n",
    "            'MAE (days)': mae_values,\n",
    "            'RMSE (days)': rmse_values,\n",
    "            'R': r2_values\n",
    "        })\n",
    "        \n",
    "        # Set index to model name\n",
    "        performance_df = performance_df.set_index('Model')\n",
    "        \n",
    "        # Display the table with styled formatting\n",
    "        from IPython.display import display\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Style for better visualization\n",
    "        styled_df = performance_df.style.background_gradient(cmap='viridis', subset=['R'])\n",
    "        styled_df = styled_df.background_gradient(cmap='coolwarm_r', subset=['MAE (days)', 'RMSE (days)'])\n",
    "        styled_df = styled_df.format({\n",
    "            'MAE (days)': '{:.2f}',\n",
    "            'RMSE (days)': '{:.2f}',\n",
    "            'R': '{:.4f}'\n",
    "        })\n",
    "        \n",
    "        display(styled_df)\n",
    "        \n",
    "        # Print best model and its performance\n",
    "        best_model = sorted_results[0][0]\n",
    "        best_mae = sorted_results[0][1]['MAE']\n",
    "        best_rmse = sorted_results[0][1]['RMSE']\n",
    "        best_r2 = sorted_results[0][1]['R2']\n",
    "        \n",
    "        print(f\"\\nBest Model: {best_model}\")\n",
    "        print(f\"Mean Absolute Error: {best_mae:.2f} days\")\n",
    "        print(f\"Root Mean Squared Error: {best_rmse:.2f} days\")\n",
    "        print(f\"R Score: {best_r2:.4f}\")\n",
    "        print(f\"\\nInterpretation: On average, predictions will be off by approximately {best_mae:.2f} days.\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # Final evaluation on test set\n",
    "        print(\"Evaluating final model on test set...\")\n",
    "        y_pred = self.model.predict(X_test_selected)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Final model performance - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}\")\n",
    "        \n",
    "        # Calculate discharge date accuracy\n",
    "        predictions_within_1_day = sum(abs(y_test - y_pred) <= 1) / len(y_test) * 100\n",
    "        predictions_within_2_days = sum(abs(y_test - y_pred) <= 2) / len(y_test) * 100\n",
    "        predictions_within_3_days = sum(abs(y_test - y_pred) <= 3) / len(y_test) * 100\n",
    "        \n",
    "        print(f\"Predictions within 1 day: {predictions_within_1_day:.2f}%\")\n",
    "        print(f\"Predictions within 2 days: {predictions_within_2_days:.2f}%\")\n",
    "        print(f\"Predictions within 3 days: {predictions_within_3_days:.2f}%\")\n",
    "        \n",
    "        # Store the expected column list\n",
    "        self.expected_columns = X.columns.tolist()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, patient_data):\n",
    "        \"\"\"\n",
    "        Predict the length of stay for a new patient\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        patient_data : dict or str\n",
    "            Patient data as a dictionary or JSON string\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        prediction : dict\n",
    "            Dictionary containing the predicted length of stay and estimated discharge date\n",
    "        \"\"\"\n",
    "        # Convert JSON string to dictionary if needed\n",
    "        if isinstance(patient_data, str):\n",
    "            patient_data = json.loads(patient_data)\n",
    "        \n",
    "        # Create DataFrame from patient data\n",
    "        patient_df = pd.DataFrame([patient_data])\n",
    "        \n",
    "        # Preprocess the data\n",
    "        processed_data = self._preprocess_patient_data(patient_df)\n",
    "        \n",
    "        # Feature engineering\n",
    "        processed_data_array = self.preprocessor.transform(processed_data)\n",
    "        processed_data_selected = self.feature_selector.transform(processed_data_array)\n",
    "        \n",
    "        # Make prediction\n",
    "        los_prediction = self.model.predict(processed_data_selected)[0]\n",
    "        \n",
    "        # Round to nearest integer\n",
    "        los_prediction_rounded = round(los_prediction)\n",
    "        \n",
    "        # Calculate estimated discharge date\n",
    "        visit_date = pd.to_datetime(patient_data.get('visit_date'))\n",
    "        estimated_discharge_date = visit_date + pd.Timedelta(days=los_prediction_rounded)\n",
    "        \n",
    "        # Calculate confidence interval based on historical data for the diagnosis\n",
    "        diagnosis = patient_data.get('primary_diagnosis')\n",
    "        std_dev = 2.0  # Default standard deviation\n",
    "        \n",
    "        if diagnosis in self.diagnosis_stats['std_los']:\n",
    "            std_dev = self.diagnosis_stats['std_los'][diagnosis]\n",
    "        \n",
    "        lower_bound = max(1, round(los_prediction - 1.96 * std_dev))\n",
    "        upper_bound = round(los_prediction + 1.96 * std_dev)\n",
    "        \n",
    "        # Create prediction result\n",
    "        result = {\n",
    "            'predicted_los': los_prediction_rounded,\n",
    "            'estimated_discharge_date': estimated_discharge_date.strftime('%Y-%m-%d'),\n",
    "            'confidence_interval': {\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound\n",
    "            },\n",
    "            'earliest_discharge': (visit_date + pd.Timedelta(days=lower_bound)).strftime('%Y-%m-%d'),\n",
    "            'latest_discharge': (visit_date + pd.Timedelta(days=upper_bound)).strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _preprocess_patient_data(self, patient_df):\n",
    "        \"\"\"\n",
    "        Preprocess patient data for prediction\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        patient_df : pandas DataFrame\n",
    "            DataFrame containing patient data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        processed_df : pandas DataFrame\n",
    "            Processed DataFrame ready for prediction\n",
    "        \"\"\"\n",
    "        # Create a copy\n",
    "        df = patient_df.copy()\n",
    "        \n",
    "        # Convert date columns to datetime\n",
    "        if 'visit_date' in df.columns:\n",
    "            df['visit_date'] = pd.to_datetime(df['visit_date'])\n",
    "            \n",
    "            # Extract datetime features\n",
    "            df['visit_year'] = df['visit_date'].dt.year\n",
    "            df['visit_month'] = df['visit_date'].dt.month\n",
    "            df['visit_day'] = df['visit_date'].dt.day\n",
    "            df['visit_hour'] = df['visit_date'].dt.hour\n",
    "            df['visit_weekday'] = df['visit_date'].dt.weekday\n",
    "            df['visit_is_weekend'] = df['visit_weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "            \n",
    "            # Remove the datetime column to avoid issues\n",
    "            df = df.drop(columns=['visit_date'])\n",
    "        \n",
    "        # Handle missing fields with appropriate defaults\n",
    "        available_fields = set(df.columns)\n",
    "        essential_fields = {\n",
    "            'age': 55,                # Median age as fallback\n",
    "            'gender_encoded': 1,      # Default to male\n",
    "            'severity_score': 3,      # Default to medium severity\n",
    "            'is_surgical_encoded': 0, # Default to non-surgical\n",
    "            'num_comorbidities': 0,   # Default to no comorbidities\n",
    "            'ward_occupancy_pct': 80  # Default ward occupancy\n",
    "        }\n",
    "        \n",
    "        # Add default values for missing essential fields\n",
    "        for field, default_value in essential_fields.items():\n",
    "            base_field = field.replace('_encoded', '')\n",
    "            if base_field in available_fields and field not in available_fields:\n",
    "                # Handle encoding for gender and is_surgical\n",
    "                if field == 'gender_encoded':\n",
    "                    df[field] = df[base_field].map({'M': 1, 'F': 0})\n",
    "                    df = df.drop(columns=[base_field])\n",
    "                elif field == 'is_surgical_encoded':\n",
    "                    df[field] = df[base_field].map({'Yes': 1, 'No': 0})\n",
    "                    df = df.drop(columns=[base_field])\n",
    "            elif field not in available_fields:\n",
    "                df[field] = default_value\n",
    "        \n",
    "        # Handle comorbidities\n",
    "        if 'comorbidities' in available_fields:\n",
    "            if 'num_comorbidities' not in available_fields:\n",
    "                df['num_comorbidities'] = df['comorbidities'].apply(\n",
    "                    lambda x: 0 if x == 'None' or pd.isna(x) else len(str(x).split(','))\n",
    "                )\n",
    "            \n",
    "            # Create binary flags for top comorbidities\n",
    "            for comorbidity in self.top_comorbidities:\n",
    "                col_name = f'has_{comorbidity.lower().replace(\" \", \"_\")}'\n",
    "                df[col_name] = df['comorbidities'].apply(\n",
    "                    lambda x: 1 if comorbidity in str(x) else 0\n",
    "                )\n",
    "            \n",
    "            # We've extracted what we need, so drop the original column\n",
    "            df = df.drop(columns=['comorbidities'])\n",
    "        else:\n",
    "            df['num_comorbidities'] = 0\n",
    "            for comorbidity in self.top_comorbidities:\n",
    "                col_name = f'has_{comorbidity.lower().replace(\" \", \"_\")}'\n",
    "                df[col_name] = 0\n",
    "        \n",
    "        # Handle diagnosis-specific features\n",
    "        if 'primary_diagnosis' in available_fields:\n",
    "            diagnosis = df['primary_diagnosis'].iloc[0]\n",
    "            \n",
    "            # Map diagnosis to its statistics\n",
    "            df['diagnosis_avg_los'] = self.diagnosis_stats['avg_los'].get(diagnosis, 9.57)  # Default to average LOS\n",
    "            df['diagnosis_std_los'] = self.diagnosis_stats['std_los'].get(diagnosis, 5.0)   # Default std deviation\n",
    "            df['diagnosis_frequency'] = self.diagnosis_stats['frequency'].get(diagnosis, 1) # Default frequency\n",
    "        else:\n",
    "            # Use overall average if diagnosis not provided\n",
    "            df['diagnosis_avg_los'] = 9.57\n",
    "            df['diagnosis_std_los'] = 5.0\n",
    "            df['diagnosis_frequency'] = 1\n",
    "        \n",
    "        # Handle vital signs with default values if missing\n",
    "        vital_signs = {\n",
    "            'heart_rate': 85,\n",
    "            'systolic_bp': 120,\n",
    "            'temperature': 37.5,\n",
    "            'oxygen_saturation': 95\n",
    "        }\n",
    "        \n",
    "        for field, default_value in vital_signs.items():\n",
    "            if field not in available_fields:\n",
    "                df[field] = default_value\n",
    "        \n",
    "        # Handle time metrics with default values if missing\n",
    "        time_fields = {\n",
    "            'laboratory_report_time_hours': 24,\n",
    "            'pharmacy_billing_time_hours': 12,\n",
    "            'insurance_claim_settlement_time_hours': 72,\n",
    "            'discharge_summary_time_hours': 7\n",
    "        }\n",
    "        \n",
    "        for field, default_value in time_fields.items():\n",
    "            base_field = field.replace('_hours', '')\n",
    "            \n",
    "            if base_field in available_fields:\n",
    "                df[field] = df[base_field].apply(self._time_to_hours)\n",
    "                df = df.drop(columns=[base_field])\n",
    "            elif field not in available_fields:\n",
    "                df[field] = default_value\n",
    "        \n",
    "        # Create interaction features\n",
    "        df['age_severity'] = df['age'] * df['severity_score']\n",
    "        df['age_comorbidities'] = df['age'] * df['num_comorbidities']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _time_to_hours(self, time_str):\n",
    "        \"\"\"Convert time string in format HH:MM:SS to decimal hours\"\"\"\n",
    "        try:\n",
    "            parts = time_str.split(':')\n",
    "            hours = int(parts[0])\n",
    "            minutes = int(parts[1])\n",
    "            seconds = int(parts[2])\n",
    "            return hours + minutes/60 + seconds/3600\n",
    "        except Exception:\n",
    "            return 0  # Default to 0 if there's an error\n",
    "    \n",
    "    def _extract_top_comorbidities(self, df, n=10):\n",
    "        \"\"\"Extract the top n most common comorbidities from the dataset\"\"\"\n",
    "        if 'comorbidities' not in df.columns:\n",
    "            return []\n",
    "            \n",
    "        # Initialize counter for comorbidities\n",
    "        comorbidity_counts = {}\n",
    "        \n",
    "        # Count each comorbidity\n",
    "        for comorbidities in df['comorbidities']:\n",
    "            if comorbidities == 'None' or pd.isna(comorbidities):\n",
    "                continue\n",
    "                \n",
    "            for comorbidity in str(comorbidities).split(','):\n",
    "                comorbidity = comorbidity.strip()\n",
    "                if comorbidity not in comorbidity_counts:\n",
    "                    comorbidity_counts[comorbidity] = 0\n",
    "                comorbidity_counts[comorbidity] += 1\n",
    "        \n",
    "        # Sort comorbidities by frequency\n",
    "        sorted_comorbidities = sorted(comorbidity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return the top n comorbidities\n",
    "        return [item[0] for item in sorted_comorbidities[:n]]\n",
    "    \n",
    "    def _create_diagnosis_stats(self, df):\n",
    "        \"\"\"Create detailed statistics for each diagnosis\"\"\"\n",
    "        if 'primary_diagnosis' not in df.columns or 'actual_los' not in df.columns:\n",
    "            # Return default empty stats if required columns are missing\n",
    "            return {\n",
    "                'avg_los': {},\n",
    "                'median_los': {},\n",
    "                'std_los': {},\n",
    "                'min_los': {},\n",
    "                'max_los': {},\n",
    "                'frequency': {},\n",
    "                'avg_severity': {}\n",
    "            }\n",
    "            \n",
    "        # Group by diagnosis and calculate statistics\n",
    "        diagnosis_groups = df.groupby('primary_diagnosis')['actual_los']\n",
    "        \n",
    "        # Calculate various statistics\n",
    "        avg_los = diagnosis_groups.mean().to_dict()\n",
    "        median_los = diagnosis_groups.median().to_dict()\n",
    "        std_los = diagnosis_groups.std().fillna(5.0).to_dict()  # Fill NaN with default\n",
    "        min_los = diagnosis_groups.min().to_dict()\n",
    "        max_los = diagnosis_groups.max().to_dict()\n",
    "        frequency = diagnosis_groups.count().to_dict()\n",
    "        \n",
    "        # Calculate average severity by diagnosis if available\n",
    "        avg_severity = {}\n",
    "        if 'severity_score' in df.columns:\n",
    "            avg_severity = df.groupby('primary_diagnosis')['severity_score'].mean().to_dict()\n",
    "        \n",
    "        # Create a comprehensive mapping\n",
    "        diagnosis_stats = {\n",
    "            'avg_los': avg_los,\n",
    "            'median_los': median_los,\n",
    "            'std_los': std_los,\n",
    "            'min_los': min_los,\n",
    "            'max_los': max_los,\n",
    "            'frequency': frequency,\n",
    "            'avg_severity': avg_severity\n",
    "        }\n",
    "        \n",
    "        return diagnosis_stats\n",
    "    \n",
    "    def save_model(self, output_path):\n",
    "        \"\"\"\n",
    "        Save the trained model and preprocessing components\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_path : str\n",
    "            Path to save the model\n",
    "        \"\"\"\n",
    "        model_data = {\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'feature_selector': self.feature_selector,\n",
    "            'model': self.model,\n",
    "            'selected_features': self.selected_features,\n",
    "            'top_comorbidities': self.top_comorbidities,\n",
    "            'diagnosis_stats': self.diagnosis_stats,\n",
    "            'model_type': self.model_type\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {output_path}\")\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Load a trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_path : str\n",
    "            Path to the saved model\n",
    "        \"\"\"\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        self.preprocessor = model_data['preprocessor']\n",
    "        self.feature_selector = model_data['feature_selector']\n",
    "        self.model = model_data['model']\n",
    "        self.selected_features = model_data['selected_features']\n",
    "        self.top_comorbidities = model_data['top_comorbidities']\n",
    "        self.diagnosis_stats = model_data['diagnosis_stats']\n",
    "        self.model_type = model_data.get('model_type', 'Unknown')\n",
    "        \n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b07e79-d19a-4420-95a2-41dd688f3a2b",
   "metadata": {},
   "source": [
    "# ## 8. Model Visualization and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba785c36-5c55-436a-8e14-1a55a80ccdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_performance(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize model performance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True length of stay values\n",
    "    y_pred : array-like\n",
    "        Predicted length of stay values\n",
    "    \"\"\"\n",
    "    # Set up figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    axes[0, 0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[0, 0].plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--')\n",
    "    axes[0, 0].set_xlabel('Actual LOS')\n",
    "    axes[0, 0].set_ylabel('Predicted LOS')\n",
    "    axes[0, 0].set_title('Actual vs Predicted LOS')\n",
    "    \n",
    "    # Plot 2: Residuals\n",
    "    residuals = y_pred - y_true\n",
    "    axes[0, 1].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Predicted LOS')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residual Plot')\n",
    "    \n",
    "    # Plot 3: Histogram of residuals\n",
    "    axes[1, 0].hist(residuals, bins=20, alpha=0.7)\n",
    "    axes[1, 0].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Residual')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Residual Distribution')\n",
    "    \n",
    "    # Plot 4: Error distribution by actual LOS\n",
    "    error_df = pd.DataFrame({\n",
    "        'actual_los': y_true,\n",
    "        'abs_error': np.abs(residuals)\n",
    "    })\n",
    "    error_by_los = error_df.groupby('actual_los')['abs_error'].mean().reset_index()\n",
    "    axes[1, 1].bar(error_by_los['actual_los'], error_by_los['abs_error'])\n",
    "    axes[1, 1].set_xlabel('Actual LOS')\n",
    "    axes[1, 1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1, 1].set_title('Error by Actual LOS')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_feature_importance(model, feature_names, model_type):\n",
    "    \"\"\"\n",
    "    Visualize feature importance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained model\n",
    "        The model to visualize\n",
    "    feature_names : list\n",
    "        Names of the features\n",
    "    model_type : str\n",
    "        Type of model\n",
    "    \"\"\"\n",
    "    # Extract feature importance\n",
    "    if model_type in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']:\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "        else:\n",
    "            print(f\"Model {model_type} does not have feature_importances_ attribute\")\n",
    "            return\n",
    "    elif model_type in ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet']:\n",
    "        if hasattr(model, 'coef_'):\n",
    "            importances = np.abs(model.coef_)\n",
    "        else:\n",
    "            print(f\"Model {model_type} does not have coef_ attribute\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"Feature importance visualization not supported for {model_type}\")\n",
    "        return\n",
    "    \n",
    "    # Ensure we have the right number of feature names\n",
    "    if len(feature_names) != len(importances):\n",
    "        # Use generic feature names\n",
    "        feature_names = [f'Feature {i}' for i in range(len(importances))]\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title(f'Feature Importance - {model_type}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_discharge_prediction(patient_data, prediction):\n",
    "    \"\"\"\n",
    "    Visualize the discharge date prediction with confidence interval\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patient_data : dict\n",
    "        Patient data\n",
    "    prediction : dict\n",
    "        Prediction results\n",
    "    \"\"\"\n",
    "    # Parse dates\n",
    "    visit_date = pd.to_datetime(patient_data['visit_date'])\n",
    "    discharge_date = pd.to_datetime(prediction['estimated_discharge_date'])\n",
    "    earliest_discharge = pd.to_datetime(prediction['earliest_discharge'])\n",
    "    latest_discharge = pd.to_datetime(prediction['latest_discharge'])\n",
    "    \n",
    "    # Create date range\n",
    "    date_range = pd.date_range(start=visit_date, end=latest_discharge + pd.Timedelta(days=1))\n",
    "    \n",
    "    # Create confidence scores - normal distribution centered at predicted discharge date\n",
    "    confidence = []\n",
    "    days_from_admission = [(date - visit_date).days for date in date_range]\n",
    "    peak_day = (discharge_date - visit_date).days\n",
    "    lower_day = (earliest_discharge - visit_date).days\n",
    "    upper_day = (latest_discharge - visit_date).days\n",
    "    \n",
    "    for day in days_from_admission:\n",
    "        # Use normal distribution for confidence\n",
    "        if day <= lower_day or day >= upper_day:\n",
    "            conf = 0.1  # Low confidence outside the interval\n",
    "        else:\n",
    "            # Higher confidence near prediction, lower as we move away\n",
    "            conf = np.exp(-0.5 * ((day - peak_day) / ((upper_day - lower_day) / 4)) ** 2)\n",
    "        confidence.append(conf)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(date_range, confidence, width=0.8, alpha=0.7, color='skyblue')\n",
    "    \n",
    "    # Highlight the predicted discharge date\n",
    "    discharge_idx = (discharge_date - visit_date).days\n",
    "    bars[discharge_idx].set_color('blue')\n",
    "    \n",
    "    # Add vertical lines for the confidence interval\n",
    "    plt.axvline(x=earliest_discharge, color='orange', linestyle='--', \n",
    "                label=f'95% CI Lower ({earliest_discharge.strftime(\"%Y-%m-%d\")})')\n",
    "    plt.axvline(x=latest_discharge, color='orange', linestyle='--',\n",
    "                label=f'95% CI Upper ({latest_discharge.strftime(\"%Y-%m-%d\")})')\n",
    "    plt.axvline(x=discharge_date, color='red', linestyle='-',\n",
    "                label=f'Predicted ({discharge_date.strftime(\"%Y-%m-%d\")})')\n",
    "    \n",
    "    # Add admission date\n",
    "    plt.axvline(x=visit_date, color='green', linestyle='-',\n",
    "                label=f'Admission ({visit_date.strftime(\"%Y-%m-%d\")})')\n",
    "    \n",
    "    # Labels and formatting\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.title(f'Discharge Date Prediction for Patient MRN: {patient_data[\"mrn\"]}')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Patient MRN: {patient_data['mrn']}\")\n",
    "    print(f\"Primary Diagnosis: {patient_data['primary_diagnosis']}\")\n",
    "    print(f\"Admission Date: {visit_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Predicted Length of Stay: {prediction['predicted_los']} days\")\n",
    "    print(f\"Predicted Discharge: {discharge_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"95% Confidence Interval: {earliest_discharge.strftime('%Y-%m-%d')} to {latest_discharge.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac17fb-ac57-461e-8ee6-d30627778c1d",
   "metadata": {},
   "source": [
    "# ## 9. Testing and Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e509980-b91f-48a2-b92e-75751ae30c6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to PostgreSQL database!\n",
      "Loaded 14001 records from database\n",
      "Training with 14001 valid records\n",
      "Training with 14 numerical features and 9 categorical features\n",
      "Training Linear Regression...\n",
      "Linear Regression - MAE: 3.26, RMSE: 4.43, R2: 0.6046\n",
      "Training Ridge...\n",
      "Ridge - MAE: 3.24, RMSE: 4.41, R2: 0.6081\n",
      "Training Lasso...\n",
      "Lasso - MAE: 5.06, RMSE: 6.88, R2: 0.0457\n",
      "Training ElasticNet...\n",
      "ElasticNet - MAE: 5.03, RMSE: 6.87, R2: 0.0489\n",
      "Training Random Forest...\n",
      "Random Forest - MAE: 3.26, RMSE: 4.52, R2: 0.5884\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting - MAE: 3.18, RMSE: 4.35, R2: 0.6182\n",
      "Training XGBoost...\n",
      "XGBoost - MAE: 3.19, RMSE: 4.44, R2: 0.6025\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1742\n",
      "[LightGBM] [Info] Number of data points in the train set: 11200, number of used features: 158\n",
      "[LightGBM] [Info] Start training from score 9.617500\n",
      "LightGBM - MAE: 3.19, RMSE: 4.42, R2: 0.6065\n",
      "Training SVR...\n",
      "SVR - MAE: 3.44, RMSE: 5.04, R2: 0.4883\n",
      "Training KNN...\n",
      "KNN - MAE: 5.01, RMSE: 6.92, R2: 0.0337\n",
      "\n",
      "Models ranked by MAE:\n",
      "1. Gradient Boosting - MAE: 3.18, RMSE: 4.35, R2: 0.6182\n",
      "2. LightGBM - MAE: 3.19, RMSE: 4.42, R2: 0.6065\n",
      "3. XGBoost - MAE: 3.19, RMSE: 4.44, R2: 0.6025\n",
      "4. Ridge - MAE: 3.24, RMSE: 4.41, R2: 0.6081\n",
      "5. Linear Regression - MAE: 3.26, RMSE: 4.43, R2: 0.6046\n",
      "6. Random Forest - MAE: 3.26, RMSE: 4.52, R2: 0.5884\n",
      "7. SVR - MAE: 3.44, RMSE: 5.04, R2: 0.4883\n",
      "8. KNN - MAE: 5.01, RMSE: 6.92, R2: 0.0337\n",
      "9. ElasticNet - MAE: 5.03, RMSE: 6.87, R2: 0.0489\n",
      "10. Lasso - MAE: 5.06, RMSE: 6.88, R2: 0.0457\n",
      "\n",
      "=== SELECTED MODEL: Gradient Boosting ===\n",
      "Best model metrics - MAE: 3.18, RMSE: 4.35, R2: 0.6182\n",
      "Model saved to feature_bridge_model.pkl\n",
      "\n",
      "Testing with sample patient data:\n",
      "{\n",
      "  \"mrn\": \"12345\",\n",
      "  \"visit_date\": \"2025-01-31 21:25:00\",\n",
      "  \"primary_diagnosis\": \"Pneumonia\",\n",
      "  \"age\": 65,\n",
      "  \"gender\": \"M\",\n",
      "  \"comorbidities\": \"\"\n",
      "}\n",
      "\n",
      "Prediction Results:\n",
      "Predicted Length of Stay: 7 days\n",
      "Estimated Discharge Date: 2025-02-07\n",
      "Earliest Possible Discharge: 2025-02-01\n",
      "Latest Possible Discharge: 2025-02-14\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 9. Feature Bridge for Using Limited JSON Input with Full Model (Fixed for Pickling)\n",
    "\n",
    "# Update database connection parameters\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"ward_data\"\n",
    "DB_USER = \"kshitizbhatnagar\"\n",
    "DB_PASSWORD = \"\"  # Empty string for no password\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create a connection to the PostgreSQL database with proper handling for no password\"\"\"\n",
    "    try:\n",
    "        # Construct connection string differently based on whether password is provided\n",
    "        if DB_PASSWORD:\n",
    "            connection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "        else:\n",
    "            connection_string = f\"postgresql://{DB_USER}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "            \n",
    "        engine = create_engine(connection_string)\n",
    "        print(\"Successfully connected to PostgreSQL database!\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample JSON fields - the limited input we have for prediction\n",
    "sample_patient_json = {\n",
    "  \"mrn\": \"12345\",\n",
    "  \"visit_date\": \"2025-01-31 21:25:00\",\n",
    "  \"primary_diagnosis\": \"Pneumonia\",\n",
    "  \"age\": 65,\n",
    "  \"gender\": \"M\",\n",
    "  \"comorbidities\": \"\"\n",
    "}\n",
    "\n",
    "# Define the FeatureBridgePredictor class in the global scope so it can be pickled\n",
    "class FeatureBridgePredictor:\n",
    "    \"\"\"\n",
    "    Predictor that bridges between limited JSON input and full feature set\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.full_model = None\n",
    "        self.feature_stats = {}\n",
    "        self.diagnosis_stats = {}\n",
    "        self.features_by_diagnosis = {}\n",
    "        self.input_columns = list(sample_patient_json.keys())\n",
    "        self.expected_columns = []\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"Train the model and compute feature statistics\"\"\"\n",
    "        # Compute statistics for each numeric feature grouped by diagnosis\n",
    "        # This will help us predict missing features based on diagnosis\n",
    "        features_by_diagnosis = {}\n",
    "        \n",
    "        if 'primary_diagnosis' in df.columns:\n",
    "            # Get a list of all numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "            \n",
    "            # Exclude certain columns from feature estimation\n",
    "            exclude_cols = ['mrn', 'visit_id', 'actual_los']\n",
    "            numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "            \n",
    "            # Compute statistics for each feature by diagnosis\n",
    "            diagnoses = df['primary_diagnosis'].unique()\n",
    "            for diagnosis in diagnoses:\n",
    "                diagnosis_df = df[df['primary_diagnosis'] == diagnosis]\n",
    "                features_by_diagnosis[diagnosis] = {}\n",
    "                \n",
    "                for col in numeric_cols:\n",
    "                    if col in diagnosis_df.columns:\n",
    "                        features_by_diagnosis[diagnosis][col] = {\n",
    "                            'mean': diagnosis_df[col].mean(),\n",
    "                            'median': diagnosis_df[col].median(),\n",
    "                            'std': diagnosis_df[col].std()\n",
    "                        }\n",
    "        \n",
    "        # Also compute general statistics for each feature\n",
    "        feature_stats = {}\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                feature_stats[col] = {\n",
    "                    'mean': df[col].mean(),\n",
    "                    'median': df[col].median(),\n",
    "                    'std': df[col].std()\n",
    "                }\n",
    "        \n",
    "        # Compute diagnosis-specific statistics\n",
    "        diagnosis_stats = {}\n",
    "        if 'primary_diagnosis' in df.columns:\n",
    "            diagnosis_groups = df.groupby('primary_diagnosis')['actual_los']\n",
    "            diagnosis_stats = {\n",
    "                'avg_los': diagnosis_groups.mean().to_dict(),\n",
    "                'median_los': diagnosis_groups.median().to_dict(),\n",
    "                'std_los': diagnosis_groups.std().fillna(2.0).to_dict(),\n",
    "                'count': diagnosis_groups.count().to_dict()\n",
    "            }\n",
    "        \n",
    "        # Store the statistics\n",
    "        self.feature_stats = feature_stats\n",
    "        self.features_by_diagnosis = features_by_diagnosis\n",
    "        self.diagnosis_stats = diagnosis_stats\n",
    "        \n",
    "        # Train a model on the full data\n",
    "        # Define features and target\n",
    "        X = df.drop(columns=['actual_los', 'mrn'] if 'mrn' in df.columns else ['actual_los'])\n",
    "        y = df['actual_los']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Identify categorical and numerical columns\n",
    "        categorical_features = [col for col in X.columns if X[col].dtype == 'object']\n",
    "        numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "        \n",
    "        print(f\"Training with {len(numerical_features)} numerical features and {len(categorical_features)} categorical features\")\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='median')),\n",
    "                    ('scaler', StandardScaler())\n",
    "                ]), numerical_features),\n",
    "                ('cat', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                ]), categorical_features)\n",
    "            ],\n",
    "            remainder='drop'\n",
    "        )\n",
    "        \n",
    "        # Define models to evaluate\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge': Ridge(),\n",
    "            'Lasso': Lasso(),\n",
    "            'ElasticNet': ElasticNet(),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "            'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "            'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42),\n",
    "            'SVR': SVR(kernel='rbf'),\n",
    "            'KNN': KNeighborsRegressor(n_neighbors=5)\n",
    "        }\n",
    "        \n",
    "        # Dictionary to store model results\n",
    "        results = {}\n",
    "        trained_models = {}\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        for name, model in models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            \n",
    "            # Create pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('model', model)\n",
    "            ])\n",
    "            \n",
    "            # Train the model\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            trained_models[name] = pipeline\n",
    "            \n",
    "            # Make predictions on test set\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'R2': r2\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}\")\n",
    "        \n",
    "        # Find the best model\n",
    "        sorted_models = sorted(results.items(), key=lambda x: x[1]['MAE'])\n",
    "        best_model_name, best_metrics = sorted_models[0]\n",
    "        \n",
    "        print(\"\\nModels ranked by MAE:\")\n",
    "        for i, (name, metrics) in enumerate(sorted_models, 1):\n",
    "            print(f\"{i}. {name} - MAE: {metrics['MAE']:.2f}, RMSE: {metrics['RMSE']:.2f}, R2: {metrics['R2']:.4f}\")\n",
    "        \n",
    "        print(f\"\\n=== SELECTED MODEL: {best_model_name} ===\")\n",
    "        print(f\"Best model metrics - MAE: {best_metrics['MAE']:.2f}, RMSE: {best_metrics['RMSE']:.2f}, R2: {best_metrics['R2']:.4f}\")\n",
    "        \n",
    "        # Store the best model\n",
    "        self.full_model = trained_models[best_model_name]\n",
    "        self.model_type = best_model_name\n",
    "        \n",
    "        # Store the expected column list\n",
    "        self.expected_columns = X.columns.tolist()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_from_json(self, patient_json):\n",
    "        \"\"\"\n",
    "        Predict LOS using only the fields in the JSON input\n",
    "        by building a bridge to the full feature set\n",
    "        \"\"\"\n",
    "        # Check if model exists\n",
    "        if self.full_model is None:\n",
    "            raise ValueError(\"Model has not been trained. Call fit() before making predictions.\")\n",
    "            \n",
    "        # Convert JSON to DataFrame\n",
    "        patient_data = pd.DataFrame([patient_json])\n",
    "        \n",
    "        # Extract basic information\n",
    "        visit_date = pd.to_datetime(patient_json.get('visit_date'))\n",
    "        diagnosis = patient_json.get('primary_diagnosis', '')\n",
    "        age = patient_json.get('age', 50)  # Default age if not provided\n",
    "        gender = patient_json.get('gender', 'M')  # Default gender if not provided\n",
    "        comorbidities = patient_json.get('comorbidities', '')\n",
    "        \n",
    "        # Get diagnosis-specific feature values or fallback to overall medians\n",
    "        # This is the key part of our feature bridge\n",
    "        full_features = {}\n",
    "        \n",
    "        # Start with features we directly have\n",
    "        full_features['age'] = age\n",
    "        full_features['gender'] = gender\n",
    "        \n",
    "        # Add date-related features\n",
    "        full_features['visit_year'] = visit_date.year\n",
    "        full_features['visit_month'] = visit_date.month\n",
    "        full_features['visit_day'] = visit_date.day\n",
    "        full_features['visit_hour'] = visit_date.hour\n",
    "        full_features['visit_weekday'] = visit_date.weekday()\n",
    "        full_features['visit_is_weekend'] = 1 if visit_date.weekday() >= 5 else 0\n",
    "        \n",
    "        # Add comorbidity features\n",
    "        full_features['comorbidities'] = comorbidities\n",
    "        full_features['num_comorbidities'] = len(comorbidities.split(',')) if comorbidities and comorbidities != 'None' else 0\n",
    "        \n",
    "        # Add diagnosis\n",
    "        full_features['primary_diagnosis'] = diagnosis\n",
    "        \n",
    "        # Add estimated values for all other numeric features based on diagnosis\n",
    "        if diagnosis in self.features_by_diagnosis:\n",
    "            diagnosis_features = self.features_by_diagnosis[diagnosis]\n",
    "            \n",
    "            for feature, stats in diagnosis_features.items():\n",
    "                if feature not in full_features:\n",
    "                    # Use median as it's more robust\n",
    "                    full_features[feature] = stats['median']\n",
    "        else:\n",
    "            # If diagnosis not found, use overall feature statistics\n",
    "            for feature, stats in self.feature_stats.items():\n",
    "                if feature not in full_features:\n",
    "                    full_features[feature] = stats['median']\n",
    "        \n",
    "        # Create a DataFrame with all expected columns\n",
    "        bridged_df = pd.DataFrame([full_features])\n",
    "        \n",
    "        # Make sure all expected columns are present\n",
    "        for col in self.expected_columns:\n",
    "            if col not in bridged_df.columns:\n",
    "                # Add with a default value\n",
    "                if col in self.feature_stats:\n",
    "                    bridged_df[col] = self.feature_stats[col]['median']\n",
    "                else:\n",
    "                    # For categorical features, use empty string\n",
    "                    bridged_df[col] = ''\n",
    "        \n",
    "        # The prediction now uses the pipeline that includes preprocessing\n",
    "        # so we don't need to preprocess the data here\n",
    "        los_prediction = self.full_model.predict(bridged_df)[0]\n",
    "        los_prediction_rounded = round(los_prediction)\n",
    "        \n",
    "        # Calculate estimated discharge date\n",
    "        discharge_date = visit_date + pd.Timedelta(days=los_prediction_rounded)\n",
    "        \n",
    "        # Calculate confidence interval\n",
    "        std_dev = self.diagnosis_stats['std_los'].get(diagnosis, 2.0) if diagnosis in self.diagnosis_stats.get('std_los', {}) else 2.0\n",
    "        \n",
    "        lower_bound = max(1, round(los_prediction - 1.96 * std_dev))\n",
    "        upper_bound = round(los_prediction + 1.96 * std_dev)\n",
    "        \n",
    "        # Create result dictionary\n",
    "        prediction = {\n",
    "            'predicted_los': los_prediction_rounded,\n",
    "            'estimated_discharge_date': discharge_date.strftime('%Y-%m-%d'),\n",
    "            'earliest_discharge': (visit_date + pd.Timedelta(days=lower_bound)).strftime('%Y-%m-%d'),\n",
    "            'latest_discharge': (visit_date + pd.Timedelta(days=upper_bound)).strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "# Function to build a feature bridge and test it\n",
    "def build_feature_bridge_and_test(model_path=None):\n",
    "    \"\"\"\n",
    "    Build a feature bridge that maps limited JSON input to full model features,\n",
    "    train a model, and test it with the sample data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_path : str\n",
    "        Path to save/load the model\n",
    "    \"\"\"\n",
    "    # Get database connection\n",
    "    engine = get_db_connection()\n",
    "    if engine is None:\n",
    "        print(\"Failed to connect to database. Exiting.\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load data from database\n",
    "        query = \"SELECT * FROM hospital_data\"\n",
    "        df = pd.read_sql_query(query, engine)\n",
    "        print(f\"Loaded {len(df)} records from database\")\n",
    "        \n",
    "        # Process datetime columns consistently\n",
    "        datetime_cols = ['visit_date', 'actual_discharge_date', 'expected_discharge_date']\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        # Create date features from visit_date\n",
    "        if 'visit_date' in df.columns:\n",
    "            df['visit_year'] = df['visit_date'].dt.year\n",
    "            df['visit_month'] = df['visit_date'].dt.month\n",
    "            df['visit_day'] = df['visit_date'].dt.day\n",
    "            df['visit_hour'] = df['visit_date'].dt.hour\n",
    "            df['visit_weekday'] = df['visit_date'].dt.weekday\n",
    "            df['visit_is_weekend'] = df['visit_weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "            df = df.drop(columns=['visit_date'])\n",
    "        \n",
    "        # Drop other datetime columns that aren't needed for prediction\n",
    "        if 'actual_discharge_date' in df.columns:\n",
    "            df = df.drop(columns=['actual_discharge_date'])\n",
    "        if 'expected_discharge_date' in df.columns:\n",
    "            df = df.drop(columns=['expected_discharge_date'])\n",
    "        \n",
    "        # Make sure the target variable exists\n",
    "        if 'actual_los' not in df.columns:\n",
    "            print(\"Error: 'actual_los' (target variable) is missing in the database!\")\n",
    "            return None, None\n",
    "        \n",
    "        # Clean target variable and ensure we have enough data\n",
    "        df = df.dropna(subset=['actual_los'])\n",
    "        df['actual_los'] = pd.to_numeric(df['actual_los'], errors='coerce')\n",
    "        \n",
    "        if len(df) < 10:\n",
    "            print(f\"Error: Not enough valid records. Only {len(df)} records remain.\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"Training with {len(df)} valid records\")\n",
    "        \n",
    "        # Create and train the feature bridge predictor\n",
    "        predictor = FeatureBridgePredictor()\n",
    "        predictor.fit(df)\n",
    "        \n",
    "        # Save the model if path provided\n",
    "        if model_path:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(predictor, f)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "        # Test with the sample patient data\n",
    "        print(\"\\nTesting with sample patient data:\")\n",
    "        print(json.dumps(sample_patient_json, indent=2))\n",
    "        \n",
    "        prediction = predictor.predict_from_json(sample_patient_json)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nPrediction Results:\")\n",
    "        print(f\"Predicted Length of Stay: {prediction['predicted_los']} days\")\n",
    "        print(f\"Estimated Discharge Date: {prediction['estimated_discharge_date']}\")\n",
    "        print(f\"Earliest Possible Discharge: {prediction['earliest_discharge']}\")\n",
    "        print(f\"Latest Possible Discharge: {prediction['latest_discharge']}\")\n",
    "        \n",
    "        return predictor, prediction\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Run the function\n",
    "predictor, prediction = build_feature_bridge_and_test('feature_bridge_model.pkl')\n",
    "\n",
    "# Create a simple visualization if prediction is available\n",
    "if prediction:\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Parse dates\n",
    "        visit_date = pd.to_datetime(sample_patient_json['visit_date'])\n",
    "        discharge_date = pd.to_datetime(prediction['estimated_discharge_date'])\n",
    "        earliest_date = pd.to_datetime(prediction['earliest_discharge'])\n",
    "        latest_date = pd.to_datetime(prediction['latest_discharge'])\n",
    "        \n",
    "        # Create date range for plotting\n",
    "        date_range = pd.date_range(visit_date, latest_date + pd.Timedelta(days=1))\n",
    "        \n",
    "        # Create bar heights (simple triangular distribution)\n",
    "        heights = []\n",
    "        for date in date_range:\n",
    "            days_from_admission = (date - visit_date).days\n",
    "            target_day = (discharge_date - visit_date).days\n",
    "            \n",
    "            # Higher near predicted date, lower as we move away\n",
    "            if days_from_admission <= target_day:\n",
    "                height = days_from_admission / max(1, target_day)\n",
    "            else:\n",
    "                days_remaining = (latest_date - date).days\n",
    "                total_days_after = (latest_date - discharge_date).days\n",
    "                height = days_remaining / max(1, total_days_after) if total_days_after > 0 else 0\n",
    "            \n",
    "            heights.append(height)\n",
    "        \n",
    "        # Create the visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(date_range, heights, width=0.8, alpha=0.7, color='skyblue')\n",
    "        plt.axvline(x=discharge_date, color='red', linestyle='--', label='Predicted Discharge')\n",
    "        plt.axvline(x=visit_date, color='green', linestyle='-', label='Admission')\n",
    "        plt.axvline(x=earliest_date, color='orange', linestyle=':', label='Earliest')\n",
    "        plt.axvline(x=latest_date, color='orange', linestyle=':', label='Latest')\n",
    "        \n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Likelihood')\n",
    "        plt.title('Discharge Date Prediction')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as viz_error:\n",
    "        print(f\"Could not create visualization: {str(viz_error)}\")\n",
    "\n",
    "# Function to load and use the saved model for prediction\n",
    "def predict_with_saved_model(model_path, patient_json):\n",
    "    \"\"\"\n",
    "    Load the saved model and make a prediction for a new patient\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_path : str\n",
    "        Path to the saved model\n",
    "    patient_json : dict\n",
    "        Patient data in JSON format\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    prediction : dict\n",
    "        Prediction results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the model\n",
    "        with open(model_path, 'rb') as f:\n",
    "            predictor = pickle.load(f)\n",
    "            \n",
    "        # Make prediction\n",
    "        prediction = predictor.predict_from_json(patient_json)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or making prediction: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71eb20f5-32cf-4400-bdca-b38458b3b590",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3513844695.py, line 337)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 337\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mf.write(\"\"\"\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Add this code to a new cell in your notebook\n",
    "\n",
    "# First, import the class from the external file\n",
    "import sys\n",
    "sys.path.append('.')  # Make sure the current directory is in the path\n",
    "from model_classes import FeatureBridgePredictor\n",
    "\n",
    "# Import necessary ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This assumes you have already loaded your data into a DataFrame called 'df'\n",
    "# Make sure 'df' contains your hospital data and has an 'actual_los' column\n",
    "\n",
    "# Create new predictor instance using the imported class\n",
    "predictor = FeatureBridgePredictor()\n",
    "\n",
    "# First, let's modify the fit method to explicitly create and train the model\n",
    "def custom_fit(predictor, df):\n",
    "    \"\"\"Custom fit method that ensures the model is created and trained\"\"\"\n",
    "    # Check and handle datetime columns\n",
    "    df_copy = df.copy()\n",
    "    datetime_cols = df_copy.select_dtypes(include=['datetime64']).columns\n",
    "    \n",
    "    print(f\"Found datetime columns: {list(datetime_cols)}\")\n",
    "    \n",
    "    # Convert datetime columns to numeric features\n",
    "    for col in datetime_cols:\n",
    "        print(f\"Converting datetime column: {col}\")\n",
    "        # Extract components and drop original\n",
    "        if col in df_copy.columns:\n",
    "            # Extract year, month, day, etc.\n",
    "            df_copy[f\"{col}_year\"] = df_copy[col].dt.year\n",
    "            df_copy[f\"{col}_month\"] = df_copy[col].dt.month\n",
    "            df_copy[f\"{col}_day\"] = df_copy[col].dt.day\n",
    "            df_copy[f\"{col}_dayofweek\"] = df_copy[col].dt.dayofweek\n",
    "            \n",
    "            # Drop the original datetime column\n",
    "            df_copy = df_copy.drop(columns=[col])\n",
    "    \n",
    "    # Call the original fit method to compute statistics\n",
    "    predictor.fit(df_copy)\n",
    "    \n",
    "    print(\"Creating and training a model with proper preprocessing...\")\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df_copy.drop(columns=['actual_los', 'mrn'] if 'mrn' in df_copy.columns else ['actual_los'])\n",
    "    y = df_copy['actual_los']\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
    "    numerical_cols = [col for col in X.columns if X[col].dtype != 'object']\n",
    "    \n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "    print(f\"Numerical columns (first 10): {numerical_cols[:10]}...\")\n",
    "    print(f\"Total columns: {len(categorical_cols) + len(numerical_cols)}\")\n",
    "    \n",
    "    # Create preprocessing pipeline with separate transformers for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_cols),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Create the full pipeline with preprocessor and model\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Split data (optional, but good practice)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    full_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the model in the predictor\n",
    "    predictor.full_model = full_pipeline\n",
    "    predictor.model_type = \"GradientBoostingWithPreprocessing\"\n",
    "    \n",
    "    # Update expected columns if needed\n",
    "    predictor.expected_columns = X.columns.tolist()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    y_pred = full_pipeline.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"Model trained! Test MAE: {mae:.2f} days\")\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "# Fit the model with our custom function\n",
    "predictor = custom_fit(predictor, df)\n",
    "\n",
    "# Save the model to a new pickle file\n",
    "import pickle\n",
    "with open(\"feature_bridge_model_new.pkl\", \"wb\") as f:\n",
    "    pickle.dump(predictor, f)\n",
    "\n",
    "print(\"Model trained and saved to feature_bridge_model_new.pkl\")\n",
    "\n",
    "# Test the model with sample data to ensure it works\n",
    "sample_patient = {\n",
    "    \"mrn\": \"12345\",\n",
    "    \"visit_date\": \"2025-01-31 21:25:00\",\n",
    "    \"primary_diagnosis\": \"Pneumonia\",\n",
    "    \"age\": 65,\n",
    "    \"gender\": \"M\",\n",
    "    \"comorbidities\": \"Diabetes,Hypertension\"\n",
    "}\n",
    "\n",
    "# Update predict_from_json to handle datetime conversion\n",
    "def modified_predict(predictor, patient_json):\n",
    "    \"\"\"Modified prediction function that handles datetime conversion\"\"\"\n",
    "    # Check if model exists\n",
    "    if predictor.full_model is None:\n",
    "        raise ValueError(\"Model has not been trained. Call fit() before making predictions.\")\n",
    "        \n",
    "    # Convert JSON to DataFrame\n",
    "    patient_data = pd.DataFrame([patient_json])\n",
    "    \n",
    "    # Extract basic information\n",
    "    visit_date = pd.to_datetime(patient_json.get('visit_date'))\n",
    "    diagnosis = patient_json.get('primary_diagnosis', '')\n",
    "    age = patient_json.get('age', 50)  # Default age if not provided\n",
    "    gender = patient_json.get('gender', 'M')  # Default gender if not provided\n",
    "    comorbidities = patient_json.get('comorbidities', '')\n",
    "    \n",
    "    # Get diagnosis-specific feature values or fallback to overall medians\n",
    "    # This is the key part of our feature bridge\n",
    "    full_features = {}\n",
    "    \n",
    "    # Start with features we directly have\n",
    "    full_features['age'] = age\n",
    "    full_features['gender'] = gender\n",
    "    \n",
    "    # Add date-related features - make sure these match how we processed in training\n",
    "    # We need to extract the same features as we did in training\n",
    "    full_features['visit_date_year'] = visit_date.year\n",
    "    full_features['visit_date_month'] = visit_date.month\n",
    "    full_features['visit_date_day'] = visit_date.day\n",
    "    full_features['visit_date_dayofweek'] = visit_date.dayofweek\n",
    "    \n",
    "    # Add comorbidity features\n",
    "    full_features['comorbidities'] = comorbidities\n",
    "    full_features['num_comorbidities'] = len(comorbidities.split(',')) if comorbidities and comorbidities != 'None' else 0\n",
    "    \n",
    "    # Add diagnosis\n",
    "    full_features['primary_diagnosis'] = diagnosis\n",
    "    \n",
    "    # Add estimated values for all other numeric features based on diagnosis\n",
    "    if diagnosis in predictor.features_by_diagnosis:\n",
    "        diagnosis_features = predictor.features_by_diagnosis[diagnosis]\n",
    "        \n",
    "        for feature, stats in diagnosis_features.items():\n",
    "            if feature not in full_features:\n",
    "                # Use median as it's more robust\n",
    "                full_features[feature] = stats['median']\n",
    "    else:\n",
    "        # If diagnosis not found, use overall feature statistics\n",
    "        for feature, stats in predictor.feature_stats.items():\n",
    "            if feature not in full_features:\n",
    "                full_features[feature] = stats['median']\n",
    "    \n",
    "    # Create a DataFrame with all expected columns\n",
    "    bridged_df = pd.DataFrame([full_features])\n",
    "    \n",
    "    # Make sure all expected columns are present\n",
    "    for col in predictor.expected_columns:\n",
    "        if col not in bridged_df.columns:\n",
    "            # Add with a default value\n",
    "            if col in predictor.feature_stats:\n",
    "                bridged_df[col] = predictor.feature_stats[col]['median']\n",
    "            else:\n",
    "                # For categorical features, use empty string\n",
    "                bridged_df[col] = ''\n",
    "    \n",
    "    # The prediction now uses the pipeline that includes preprocessing\n",
    "    # so we don't need to preprocess the data here\n",
    "    los_prediction = predictor.full_model.predict(bridged_df)[0]\n",
    "    los_prediction_rounded = round(los_prediction)\n",
    "    \n",
    "    # Calculate estimated discharge date\n",
    "    discharge_date = visit_date + pd.Timedelta(days=los_prediction_rounded)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    std_dev = predictor.diagnosis_stats['std_los'].get(diagnosis, 2.0) if diagnosis in predictor.diagnosis_stats.get('std_los', {}) else 2.0\n",
    "    \n",
    "    lower_bound = max(1, round(los_prediction - 1.96 * std_dev))\n",
    "    upper_bound = round(los_prediction + 1.96 * std_dev)\n",
    "    \n",
    "    # Create result dictionary\n",
    "    prediction = {\n",
    "        'predicted_los': los_prediction_rounded,\n",
    "        'estimated_discharge_date': discharge_date.strftime('%Y-%m-%d'),\n",
    "        'earliest_discharge': (visit_date + pd.Timedelta(days=lower_bound)).strftime('%Y-%m-%d'),\n",
    "        'latest_discharge': (visit_date + pd.Timedelta(days=upper_bound)).strftime('%Y-%m-%d')\n",
    "    }\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test with sample data\n",
    "try:\n",
    "    # Use our modified predict function to test\n",
    "    result = modified_predict(predictor, sample_patient)\n",
    "    print(\"\\nSample prediction result:\")\n",
    "    print(f\"Predicted Length of Stay: {result['predicted_los']} days\")\n",
    "    print(f\"Estimated Discharge Date: {result['estimated_discharge_date']}\")\n",
    "    print(f\"Earliest Possible Discharge: {result['earliest_discharge']}\")\n",
    "    print(f\"Latest Possible Discharge: {result['latest_discharge']}\")\n",
    "    \n",
    "    # Now update the predictor's prediction method to match our modified version\n",
    "    predictor.predict_from_json = lambda patient_json: modified_predict(predictor, patient_json)\n",
    "    \n",
    "    # Save the updated predictor\n",
    "    with open(\"feature_bridge_model_new.pkl\", \"wb\") as f:\n",
    "        pickle.dump(predictor, f)\n",
    "    \n",
    "    print(\"\\nUpdated model saved with modified prediction function\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during prediction: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4257b4-e630-440c-8493-7038044f3183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
